{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAKE REVIEW DETECTION\n",
    "\n",
    "\n",
    "## Seattle University\n",
    "## Instructor: Dr. Kim\n",
    "## Student: Huy Le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readme\n",
    "This notebook contains entire of my code about my fake review detection research. It includes from the first step to get data from \"yelpreview021302.csv\" to implement machine learning models. If your computer can not execute the whole file due to the contrains of memory, you may think about break the code to many part such as Data Preprocessing, NLP (Tokenizing, TF-IDF, Consine Similarity, Sentiment Analysis), Clustering, Classifying Model, Decision Tree Diagram Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [1. Import Libraries and Dataset](#1st-bullet)\n",
    "* [2. Summary Statistic](#2nd-bullet)\n",
    "* [3. Data Preprocessing](#3rd-bullet)\n",
    "    - [3.1 Textual Data Cleaning](#3_1-bullet)\n",
    "* [4. Genereate Text Features](#4th-bullet)\n",
    "    - [4.1 Part of Speech](#4_1-bullet)\n",
    "    - [4.2 Cosine Similarity](#4_2-bullet)\n",
    "        - [a. Cosine Similarity by N-grams](#4_2_a-bullet)\n",
    "        - [b. Cosine Similarity by Part of Speech](#4_2_b-bullet)\n",
    "    - [4.3 Sentiment analysis](#4_3-bullet)\n",
    "* [5. Clusstering](#5th-bullet)\n",
    "    - [5.1 Clustering with PoS-based cosine similarity](#5_1-bullet)\n",
    "    - [5.2 Clustering with unigram-based Cosine similarity](#5_2-bullet)\n",
    "    - [5.3 Integration cluster label with behavioral features](#5_3-bullet)\n",
    "    \n",
    "* [6. Building classify model](#6th-bullet)\n",
    "    - [6.1 Features Selection based logistic regression](#6_1-bullet)\n",
    "    - [6.2 SVM model](#6_2-bullet)\n",
    "        - [a. PoS similarity](#6_2_a-bullet)\n",
    "        - [b. Unigram similarity ](#6_2_b-bullet)\n",
    "    - [6.3 Random Forest Classifiers](#6_3-bullet)\n",
    "        - [a. PoS similarity](#6_3_a-bullet)\n",
    "        - [b. Unigram similarity ](#6_3_b-bullet)\n",
    "    - [6.4 Neural Network](#6_4-bullet)\n",
    "        - [a. PoS similarity](#6_4_a-bullet)\n",
    "        - [b. Unigram similarity ](#6_4_b-bullet)\n",
    "    - [6.5 Decision Tree ](#6_5-bullet)\n",
    "        - [a. PoS similarity](#6_5_a-bullet)\n",
    "        - [b. Unigram similarity ](#6_5_b-bullet)\n",
    "        - [c. Visualizing Decision Tree Diagram](#6_5_c-bullet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries and Dataset <a class=\"anchor\" id=\"1st-bullet\"></a>\n",
    "\n",
    "The dataset I used in this research was acquire from Dr. Liu at Unversity of Illinois at Chicago. I had done some integration and preprocessing steps in SQL before I came up to this datasets. For more detail about my dataset, pleas read my paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "nltk.download('stopwords') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "raw_data= pd.read_csv('./yelpreview021302.csv')\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# I only chose review with flagged N an Y which mean non-filterd and filterd review.\n",
    "raw_data = raw_data.loc[(raw_data['flagged'] !='NR') & (raw_data['flagged']!='YR') ]\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample with 10000 observation with 50:50 fake and non-fake review.\n",
    "#df = raw_data.sample(5000).groupby('flagged').head(5000)\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, 5000, True),:]\n",
    "df = raw_data.groupby('flagged', as_index=False).apply(fn)\n",
    "df = shuffle(df)\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['flagged','buscateg','pricerange','firstreview']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Summarize statistic <a class=\"anchor\" id=\"2nd-bullet\"></a>\n",
    "- proportion of fake and non fake in dataset\n",
    "- the corelation between \"flagged and other feature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize statistic\n",
    "print(round(df.describe(),2))\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pricerange'] = df['pricerange'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data distribution\n",
    "Y = df['flagged']\n",
    "\n",
    "print(Y.value_counts())\n",
    "\n",
    "print('% filtered reviews: {}'.format(round(Y.value_counts()[0]/len(Y)*100),3))\n",
    "print('% non-filtered reviews: {}'.format(round(Y.value_counts()[1]/len(Y)*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export dataset to CSV file \n",
    "#df.to_csv('sampleof10k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing  <a class=\"anchor\" id=\"3rd-bullet\"></a>\n",
    "## 3.1 Textual Data Cleaning\n",
    "\n",
    "steps:\n",
    "- text feature genereate: \n",
    "    - n-gram\n",
    "    - Part of Spech\n",
    "    - TF-IDF\n",
    "    - Cosine Similarity\n",
    "- sentiment analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the review content and non-text features to diff dataframe\n",
    "df['flagged'] = df['flagged'].astype('category')\n",
    "df['buscateg'] = df['buscateg'].astype('category')\n",
    "df['pricerange'] = df['pricerange'].astype('category')\n",
    "df['firstreview'] = df['firstreview'].astype('category')\n",
    "\n",
    "\n",
    "review_content = df['reviewcontent']\n",
    "behavior_attr = df[['reviewrating',\n",
    "                    'reusefulcount',\n",
    "                    'recoolcount',\n",
    "                    'refunnycount',\n",
    "                    'friendcount',\n",
    "                    'fancount',\n",
    "                    'tipcount',\n",
    "                    'reviewcount',\n",
    "                    'firstcount',\n",
    "                    'usefulcount',\n",
    "                    'coolcount',\n",
    "                    'complimentcount',\n",
    "                    'funnycount',\n",
    "                    'busrating',\n",
    "                    'buscateg',\n",
    "                    'pricerange',\n",
    "                    'monmembership',\n",
    "                    'firstreview',\n",
    "                    'maxReviewDay',\n",
    "                    'avgReviewDay',\n",
    "                    'avgpostedrating',\n",
    "                    'avgreviewlen'\n",
    "                    ]]\n",
    "\n",
    "# replace NaN with space\n",
    "review_content = review_content.fillna('')\n",
    "\n",
    "\n",
    "\n",
    "# convert class labels to binary values, 0 = ham and 1 = spam\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(df['flagged'])\n",
    "\n",
    "behavior_attr['buscateg'] =  encoder.fit_transform(behavior_attr['buscateg'])\n",
    "behavior_attr['pricerange'] = encoder.fit_transform(behavior_attr['pricerange'])\n",
    "behavior_attr['firstreview'] = encoder.fit_transform(behavior_attr['firstreview'])\n",
    "\n",
    "print(review_content[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expressions to replace email address, URLs, Phone numbers, other numbers\n",
    "\n",
    "# Replace email addresses with 'email'\n",
    "processed = review_content.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
    "                                      'emailaddress')\n",
    "\n",
    "# Replace URLs with 'webaddress'\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                   'webaddress')\n",
    "\n",
    "# Replace moneu symbols with 'moneysymb' (€,£)\n",
    "processed = processed.str.replace(r'€|\\$|£', 'moneysymb')\n",
    "\n",
    "# replace 10 digit phone numbers (formats include paranthesis, space, no space, dashes) with phonenumber\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                 'phonenumber')\n",
    "\n",
    "#replace numbers with 'number'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?','number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Punctuation\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# replace whitespac between terms with a single space\n",
    "processed = processed.str.replace(r'\\s+',' ') \n",
    "\n",
    "# remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change words to lower case\n",
    "processed = processed.str.lower()\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from review\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "            term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word stems using a Porter stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "            ps.stem(term) for term in x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed.to_csv('processed_content.csv', header ='review_content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Generate Text features <a class=\"anchor\" id=\"4th-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Part of Speech <a class=\"anchor\" id=\"4_1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create part of speech represents of review content\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#tokenized review into word\n",
    "#word_tok = processed.apply(word_tokenize)\n",
    "#print(word_tok.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [('I', 'PRP'), (\"'m\", 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]\n",
    "pos = processed.apply(word_tokenize).apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(pos):\n",
    "    pos[i] =' '.join([word + '_' + postag for word, postag in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Cosine Similarity <a class=\"anchor\" id=\"4_2-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Packages For Vectorization of Text For Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Cosine Similarity by N-grams <a class=\"anchor\" id=\"4_2_a-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = processed\n",
    "# using Uni-gram because bi-gram or trigram take too long to generate, but did show difference between reviews\n",
    "#cv = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "#X_cv = cv.fit_transform(corpus) # Fit the Data\n",
    "\n",
    "# create fi-idf vector\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1)) # create tf-idf vector based unigram, Range(1,1) mean n =[1,1]\n",
    "#X_tfidf= tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cosine similarity by tf-idf vector\n",
    "CS_similarity_bigram =cosine_similarity(tfidf.fit_transform(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature shape\n",
    "CS_similarity_bigram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CS_similarity_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del CS_similarity_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Cosine Similarity by Part of Speech <a class=\"anchor\" id=\"4_2_b-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus = pos\n",
    "# create fi-idf vector base on POS\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "pos_tfidf= tfidf.fit_transform(corpus)\n",
    "\n",
    "#print(pos_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# create cosine similarity by tf-idf vector\n",
    "CS_similarity_pos =cosine_similarity(pos_tfidf)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CS_similarity_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chunk is used to save the cosine similarity matrix\n",
    "# Use if you want separate my code to overcome memory exceed\n",
    "\n",
    "# import pickle\n",
    "# # #save variable\n",
    "# with open('CS_similarity_pos', 'wb') as f:\n",
    "#     pickle.dump(CS_similarity_pos, f)\n",
    "\n",
    "# with open('CS_similarity_bigram', 'wb') as f:\n",
    "#     pickle.dump(CS_similarity_bigram, f)\n",
    "   \n",
    "#load variable\n",
    "#with open(filename, ‘rb’) as f:\n",
    "    #var_you_want_to_load_into = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cs_matrix = CS_similarity_pos\n",
    "\n",
    "# for i in range(0, cs_matrix.shape[0]):\n",
    "#     cs_matrix[i,i] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cs_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Sentiment analysis <a class=\"anchor\" id=\"4_3-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob \n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "#Example how to get sentiment score of a text\n",
    "#rv_sentiment = TextBlob(\"Came in on the early afternoon on Sunday. The food was tasty and priced well.\",analyzer=NaiveBayesAnalyzer()).sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Note **\n",
    "Because NaiveBayesAnalyzer() take too long to analyze the sentiment of whole dataset. I had tested and it take me more than 4hours. So, I choose move forward with pattern analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Polarity and subjectivity\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "# generate polarity and subjectivity then add them to behavior_attr dataframe\n",
    "behavior_attr['polarity'] = processed.apply(pol)\n",
    "behavior_attr['subjective'] = processed.apply(sub)\n",
    "\n",
    "#reve_pos = []\n",
    "#reve_pos += [TextBlob(x, analyzer= NaiveBayesAnalyzer()).sentiment.p_pos  for x in processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#behavior_attr.to_csv('behavior_attr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the results\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "# x = polarity\n",
    "# y = subjective\n",
    "# plt.scatter(x, y, color='blue')\n",
    "# #plt.text(x+.001, y+.001, data['full_name'][index], fontsize=10)\n",
    "# plt.xlim(-.01, .12) \n",
    "    \n",
    "# plt.title('Sentiment Analysis', fontsize=20)\n",
    "# plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
    "# plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavior feature\n",
    "- behavior_attr: behavioral attributes\n",
    "### Text_feature\n",
    "- processed: text content\n",
    "- polarity: polarity of review\n",
    "- subjective: subjectivity of review\n",
    "- CS_similarity_pos: consine similarity generated from POS\n",
    "- CS_similarity_bigram: cosine similarity generated from bi-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Clusstering <a class=\"anchor\" id=\"5th-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Clustering with PoS-based cosine similarity <a class=\"anchor\" id=\"5_1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "def em_clustering(X, k ):\n",
    "    gmm = GaussianMixture(n_components=k, random_state= 138)\n",
    "    gmm.fit(X)\n",
    "    return gmm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em Clustering\n",
    "start_time = time.time()\n",
    "k= 5 \n",
    "gmm = em_clustering(CS_similarity_pos,k)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pos = gmm.predict(CS_similarity_pos)\n",
    "\n",
    "cluster_pos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot clustering result\n",
    "dt_plt = pd.DataFrame({'Cluster':cluster_pos})\n",
    "dt_plt =pd.DataFrame({'Cluster':dt_plt.groupby('Cluster')['Cluster'].count().index, 'count': dt_plt.groupby('Cluster')['Cluster'].count()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing\n",
    "import itertools\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_iter = ['navy', 'turquoise', 'cornflowerblue','darkorange']\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.bar(dt_plt['Cluster'], dt_plt['count'],  color=color_iter )\n",
    "plt.xlabel('Clusters')\n",
    "plt.xticks(dt_plt['Cluster'])\n",
    "plt.ylabel('Count')\n",
    "plt.title('Review Clustering by POS Cosine Similarity')\n",
    "\n",
    "for x,y in zip(dt_plt['Cluster'], dt_plt['count']):\n",
    "    plt.annotate('{}'.format(y ),\n",
    "                 xy=(x , y + 10),\n",
    "                 xytext=(0, 3),  # 3 points vertical offset\n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This plot visualize cluster based on the cosine similarity among the reviews\n",
    "\n",
    "plt.scatter(CS_similarity_pos[:, 0], CS_similarity_pos[:, 1], c=cluster_pos, s=40, cmap='viridis', alpha =0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "#     \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "#     ax = ax or plt.gca()\n",
    "    \n",
    "#     # Convert covariance to principal axes\n",
    "#     if covariance.shape == (2, 2):\n",
    "#         U, s, Vt = np.linalg.svd(covariance)\n",
    "#         angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "#         width, height = 2 * np.sqrt(s)\n",
    "#     else:\n",
    "#         angle = 0\n",
    "#         width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "#     # Draw the Ellipse\n",
    "#     for nsig in range(1, 4):\n",
    "#         ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "#                              angle, **kwargs))\n",
    "        \n",
    "# def plot_gmm(gmm, lables_, X, label=True, ax=None):\n",
    "#     ax = ax or plt.gca()\n",
    "#     labels = lables_\n",
    "#     if label:\n",
    "#         ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n",
    "#     else:\n",
    "#         ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n",
    "#     ax.axis('equal')\n",
    "    \n",
    "#     w_factor = 0.2 / gmm.weights_.max()\n",
    "#     for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
    "#         draw_ellipse(pos, covar, alpha=w * w_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_gmm(gmm,cluster_pos, CS_similarity_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Clustering with unigram-based Cosine similarity  <a class=\"anchor\" id=\"5_2-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "# em Clustering\n",
    "start_time = time.time()\n",
    "gmm = em_clustering(CS_similarity_bigram,k)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ngram = gmm.predict(CS_similarity_bigram)\n",
    "\n",
    "cluster_ngram[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot clustering result\n",
    "dt_plt = pd.DataFrame({'Cluster':cluster_ngram})\n",
    "dt_plt = dt_plt.groupby('Cluster')['Cluster'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualizing clusters\n",
    "\n",
    "color_iter = ['navy', 'turquoise', 'cornflowerblue','darkorange']\n",
    "\n",
    "dt_plt.plot(kind ='bar', color = color_iter)\n",
    "for x,y in zip(dt_plt.index, dt_plt):\n",
    "    plt.annotate('{}'.format(y ),\n",
    "                 xy=(x , y + 10),\n",
    "                 xytext=(0, 3),  # 3 points vertical offset\n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom')\n",
    "plt.title('CLusters by Ngram cosine similarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Integration cluster label with behavioral features   <a class=\"anchor\" id=\"5_3-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos=pd.concat([behavior_attr,\n",
    "                 pd.DataFrame({'cluster':cluster_pos}).reindex(behavior_attr.index)],\n",
    "#                  pd.DataFrame({'polarity':polarity}).reindex(behavior_attr.index),\n",
    "#                  pd.DataFrame({'subjective':subjective}).reindex(behavior_attr.index)],\n",
    "                 #pd.DataFrame(CS_similarity_bigram).reindex(behavior_attr.index)], \n",
    "                 axis=1) \n",
    "df_ngram=pd.concat([behavior_attr,\n",
    "                 pd.DataFrame({'cluster':cluster_ngram}).reindex(behavior_attr.index)],\n",
    "#                  pd.DataFrame({'polarity':polarity}).reindex(behavior_attr.index),\n",
    "#                  pd.DataFrame({'subjective':subjective}).reindex(behavior_attr.index)],\n",
    "#                  #pd.DataFrame(CS_similarity_bigram).reindex(behavior_attr.index)], \n",
    "                 axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can extract data for latter steps, so you dont have to run the entire code all the time.\n",
    "\n",
    "#df_pos.to_csv('df_pos.csv')\n",
    "#df_ngram.to_csv('df_ngram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pos.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Building classify model <a class=\"anchor\" id=\"6th-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Features Selection based logistic regression <a class=\"anchor\" id=\"6_1-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = df_pos\n",
    "y = Y\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "\n",
    "result = stepwise_selection(X, y)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_pos[['monmembership', 'maxReviewDay', 'reusefulcount', 'avgReviewDay', 'complimentcount', 'avgreviewlen', 'reviewrating', 'avgpostedrating', 'recoolcount', 'tipcount', 'buscateg', 'polarity', 'fancount', 'firstreview', 'subjective', 'cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Svm model <a class=\"anchor\" id=\"6_2-bullet\"></a>\n",
    "### a. PoS similarity <a class=\"anchor\" id=\"6_2_a-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entire dataset\n",
    "# test building classify model with consine similarity\n",
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_SVM_(X,Y, kernel ='rbf'):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20,random_state=138) \n",
    "    scaler = StandardScaler()  \n",
    "    scaler.fit(X)\n",
    "    X_train = scaler.transform(X_train)  \n",
    "    X_test = scaler.transform(X_test)\n",
    "    # SVM kernel: ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed\n",
    "    # standardize the independent variable\n",
    "    svclassifier = SVC(kernel='rbf')    \t## Linear SVM\n",
    "    svclassifier.fit(X_train, y_train)  \n",
    "    y_pred = svclassifier.predict(X_test)  \t## predict test se\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Error rate:\", 1-metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "    print('{} {} {} {}'.format(metrics.accuracy_score(y_test, y_pred),\n",
    "                   1-metrics.accuracy_score(y_test, y_pred),\n",
    "                   metrics.recall_score(y_test, y_pred),\n",
    "                   metrics.precision_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "fit_SVM_(df_pos,Y, kernel ='rbf')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,k):\n",
    "    df_cluster0 = df_pos[df_pos['cluster'] == i]\n",
    "    Y = pd.Series(Y)\n",
    "    print('\\n Classifying in cluster: {}'.format(i))\n",
    "    fit_SVM_(df_cluster0.drop('cluster',axis=1),Y.iloc[df_cluster0.index], kernel ='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withou cluster\n",
    "fit_SVM_(df_pos.drop('cluster',axis=1),Y, kernel ='rbf' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Unigram similarity <a class=\"anchor\" id=\"6_2_b-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "fit_SVM_(df_ngram,Y, kernel ='rbf')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,k):\n",
    "    df_cluster0 = df_ngram[df_ngram['cluster'] == i]\n",
    "    Y = pd.Series(Y)\n",
    "    print('Classifying in cluster: {}'.format(i))\n",
    "    fit_SVM_(df_cluster0.drop('cluster',axis=1),Y.iloc[df_cluster0.index], kernel ='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Random Forest Classifiers <a class=\"anchor\" id=\"6_3-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. PoS similarity <a class=\"anchor\" id=\"6_3_a-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RF_selction(X,Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20,random_state=138) \n",
    "    sel = SelectFromModel(RandomForestClassifier(n_estimators = 500,max_depth = 100))\n",
    "    sel.fit(X_train, y_train)\n",
    "    sel.get_support()\n",
    "    selected_feat= X_train.columns[(sel.get_support())]\n",
    "    print(len(selected_feat))\n",
    "    print(selected_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_selction(df_pos,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classifier(X,Y, \n",
    "                  max_features_= 'auto', \n",
    "                  n_estimators_ = 100,\n",
    "                  max_depth_ = None,\n",
    "                  min_sample_leaf_ = 2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20,random_state=138) \n",
    "    clf = RandomForestClassifier(max_features = max_features_, \n",
    "                                 n_estimators = n_estimators_,\n",
    "                                 max_depth = max_depth_,\n",
    "                                 min_samples_leaf = min_sample_leaf_)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)  \t## predict test set\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Error rate:\", 1-metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "    print('{} {} {} {}'.format(metrics.accuracy_score(y_test, y_pred),\n",
    "                   1-metrics.accuracy_score(y_test, y_pred),\n",
    "                   metrics.recall_score(y_test, y_pred),\n",
    "                   metrics.precision_score(y_test, y_pred)))\n",
    "    return(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RF_classifier(df_pos,Y,n_estimators_=500,min_sample_leaf_= 2, max_depth_ = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': list(df_pos.columns),\n",
    "                  'importance': clf.feature_importances_}).\\\n",
    "                   sort_values('importance', ascending = False)\n",
    "print(fi.head(10))\n",
    "\n",
    "dfplot = fi.head(10).sort_values('importance', ascending = True)\n",
    "\n",
    "N=10\n",
    "colors = np.random.rand(N)\n",
    "plt.scatter(dfplot['feature'], dfplot['importance'], s =50, c='tomato', alpha=1, marker ='^')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Features Name')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Top 10 Importance Features in RF model')\n",
    "for x,y in zip(range(0,11), dfplot['importance']):\n",
    "    plt.annotate('{}'.format(round(y,3)),\n",
    "                 xy=(x , y + 0.01),\n",
    "                 xytext=(0, 0),  # 3 points vertical offset\n",
    "                 textcoords=\"offset points\",\n",
    "                 ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,k):\n",
    "    df_cluster0 = df_pos[df_pos['cluster'] == i]\n",
    "    Y = pd.Series(Y)\n",
    "    print('\\nClassifying in cluster: {}'.format(i))\n",
    "    RF_classifier(df_cluster0.drop('cluster',axis=1),Y.iloc[df_cluster0.index],n_estimators_=500,min_sample_leaf_= 2, max_depth_ = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# withou cluster\n",
    "RF_classifier(df_pos.drop('cluster',axis=1),Y,n_estimators_=150,min_sample_leaf_= 2, max_depth_ = 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# withou text generated features\n",
    "RF_classifier(df_pos.drop(['polarity','subjective'],axis=1),Y,n_estimators_=150,min_sample_leaf_= 2, max_depth_ = 50 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Unigram similarity <a class=\"anchor\" id=\"6_3_b-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF_classifier(df_ngram,Y,n_estimators_=150,min_sample_leaf_= 2, max_depth_ = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,k):\n",
    "    df_cluster0 = df_ngram[df_ngram['cluster'] == i]\n",
    "    Y = pd.Series(Y)\n",
    "    print('\\nClassifying in cluster: {}'.format(i))\n",
    "    RF_classifier(df_cluster0.drop('cluster',axis=1),Y.iloc[df_cluster0.index],n_estimators_=150,min_sample_leaf_= 2, max_depth_ = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# withou cluster\n",
    "RF_classifier(df_ngram.drop('cluster',axis=1),Y,n_estimators_=150,min_sample_leaf_= 2, max_depth_ = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withou text generated features\n",
    "RF_classifier(df_ngram.drop(['cluster','polarity','subjective'],axis=1),Y,n_estimators_=150,min_sample_leaf_= 2, max_depth_ = 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Neural Network <a class=\"anchor\" id=\"6_4-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. PoS similarity <a class=\"anchor\" id=\"6_4_a-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_classifier(X, Y, hidden_layers_ = (5,2), activation_ ='relu',solver_ ='adam' , alphafloat_ =1e-5):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20,random_state=138)\n",
    "\n",
    "    clf = MLPClassifier(solver=solver_,\n",
    "                        alpha=alphafloat_,\n",
    "                        activation =activation_,\n",
    "                        hidden_layer_sizes= hidden_layers_,\n",
    "                        random_state=1)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    cm =  metrics.confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Error rate:\", 1-metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "    print('{} {} {} {}'.format(metrics.accuracy_score(y_test, y_pred),\n",
    "                   1-metrics.accuracy_score(y_test, y_pred),\n",
    "                   metrics.recall_score(y_test, y_pred),\n",
    "                   metrics.precision_score(y_test, y_pred)))\n",
    "#     plt.clf()\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "#     classNames = ['0','1']\n",
    "#     plt.title('NN Confusion Matrix - Test Data')\n",
    "#     plt.ylabel('True Class')\n",
    "#     plt.xlabel('Predicted Class')\n",
    "#     tick_marks = np.arange(len(classNames))\n",
    "#     plt.xticks(tick_marks, classNames, rotation=45)\n",
    "#     plt.yticks(tick_marks, classNames)\n",
    "#     s = [['TN','FP'], ['FN', 'TP']]\n",
    "#     for i in range(2):\n",
    "#         for j in range(2):\n",
    "#             plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full clusters\n",
    "\n",
    "NN_classifier(df_pos,Y,hidden_layers_ =(20,30,40,50), solver_='adam', activation_= 'tanh' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,k):\n",
    "    df_cluster0 = df_pos[df_pos['cluster'] == i]\n",
    "    Y = pd.Series(Y)\n",
    "    print('\\nClassifying in cluster: {}'.format(i))\n",
    "    NN_classifier(df_cluster0.drop('cluster',axis=1),Y.iloc[df_cluster0.index],hidden_layers_ =(20,30,40,50), solver_='adam', activation_= 'tanh' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without cluster\n",
    "NN_classifier(df_pos.drop('cluster',axis=1),Y,hidden_layers_ =(20,30,40,50), solver_='adam', activation_= 'tanh' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Unigram similarity <a class=\"anchor\" id=\"6_4_b-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NN_classifier(df_ngram,Y,hidden_layers_ =(20,30,40,50), solver_='adam', activation_= 'tanh' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,k):\n",
    "    df_cluster0 = df_ngram[df_ngram['cluster'] == i]\n",
    "    Y = pd.Series(Y)\n",
    "    print('Classifying in cluster: {}'.format(i))\n",
    "    NN_classifier(df_cluster0.drop('cluster',axis=1),Y.iloc[df_cluster0.index],hidden_layers_ =(20,30,40,50), solver_='adam', activation_= 'tanh' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# withou cluster\n",
    "NN_classifier(df_ngram.drop('cluster',axis=1),Y,hidden_layers_ =(20,30,40,50), solver_='adam', activation_= 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withou cluster\n",
    "NN_classifier(df_ngram.drop(['cluster','polarity','subjective'],axis=1),Y,hidden_layers_ =(20,30,40,50), solver_='adam', activation_= 'tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Decision Tree <a class=\"anchor\" id=\"6_5-bullet\"></a>\n",
    "\n",
    "### a. PoS similarity <a class=\"anchor\" id=\"6_5_a-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT_clf(X, Y, max_features_= 'auto', \n",
    "                  max_depth_ = None,\n",
    "                  min_sample_leaf_ = 2):\n",
    "    from sklearn import tree\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20,random_state=138)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier(max_features = max_features_, \n",
    "                                     max_depth = max_depth_,\n",
    "                                     min_samples_leaf = min_sample_leaf_)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #plot the tree\n",
    "    #tree.plot_tree(clf.fit(X_train, y_train)) \n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    cm =  metrics.confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Error rate:\", 1-metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "    print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "    plt.clf()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "    classNames = ['0','1']\n",
    "    plt.title('NN Confusion Matrix - Test Data')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames, rotation=45)\n",
    "    plt.yticks(tick_marks, classNames)\n",
    "    s = [['TN','FP'], ['FN', 'TP']]\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "    plt.show()\n",
    "    return clf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtmodel = DT_clf(df_pos,Y,min_sample_leaf_= 2, max_depth_ = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,k):\n",
    "    df_cluster0 = df_pos[df_pos['cluster'] == i]\n",
    "    Y = pd.Series(Y)\n",
    "    print('Classifying in cluster: {}'.format(i))\n",
    "    dt_k = DT_clf(df_pos.drop('cluster', axis =1),Y,min_sample_leaf_= 2, max_depth_ = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# withou cluster\n",
    "dtmodel = DT_clf(df_pos.drop(['cluster'],axis=1),Y,min_sample_leaf_= 2, max_depth_ = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Unigram similarity <a class=\"anchor\" id=\"6_5_b-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtmodel = DT_clf(df_ngram,Y,min_sample_leaf_= 2, max_depth_ = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,k):\n",
    "    df_cluster0 = df_ngram[df_ngram['cluster'] == i]\n",
    "    Y = pd.Series(Y)\n",
    "    print('Classifying in cluster: {}'.format(i))\n",
    "    dt_k = DT_clf(df_cluster0.drop('cluster', axis =1),Y[df_cluster0.index],min_sample_leaf_= 2, max_depth_ = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without cluster\n",
    "dtmodel = DT_clf(df_ngram.drop(['cluster'],axis=1),Y,min_sample_leaf_= 2, max_depth_ = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Visualizing Decision Tree Diagram <a class=\"anchor\" id=\"6_5_c-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets recreate decision tree model for visualize\n",
    "# chosing 10 level of depth \n",
    "dtmodel = DT_clf(df_ngram, Y, min_sample_leaf_= 2, max_depth_ = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "from IPython.display import SVG\n",
    "#from StringIO import StringIO\n",
    "\n",
    "dot_data= tree.export_graphviz(dtmodel, # decision tree model\n",
    "                               out_file=\"ngram_8cluster.dot\", # name of dot file \n",
    "                               feature_names=df_ngram.columns) # features name will be displayed in the tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "from IPython.display import SVG\n",
    "from sklearn import tree\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\pos_DT_8cluster.dot\") as f: # \"pos_DT_8cluster.dot\" is name of graphviz decision tree file\n",
    "    dot_data = f.read()\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.format = 'png' # file type\n",
    "graph.render(\"ngram_DT_8clusterimage\",view=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
