{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pyspark\n",
    "import math\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from time import time\n",
    "from spark_tree_plotting import plot_tree \n",
    "import json\n",
    "import bson\n",
    "from pyspark.sql import SQLContext\n",
    "#from bson import json_util\n",
    "#from bson.json_util import dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-12-d9abf4af8e6c>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-d9abf4af8e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kddcup.data.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    306\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 308\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    309\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-12-d9abf4af8e6c>:1 "
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext('local[*]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting train data\n",
    "f = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"kddcup.data.gz\")\n",
    "\n",
    "\n",
    "data_file = \"./kddcup.data.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "print (\"Train data size is {}\".format(raw_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size is 311029\n"
     ]
    }
   ],
   "source": [
    "# getting test data\n",
    "ft = urllib.request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"corrected.gz\")\n",
    "\n",
    "\n",
    "test_data_file = \"./corrected.gz\"\n",
    "test_raw_data = sc.textFile(test_data_file)\n",
    "\n",
    "print(\"Test data size is {}\".format(test_raw_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,215,45076,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,0,0,0.00,0.00,0.00,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,162,4528,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,1,1,1.00,0.00,1.00,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,236,1228,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,2,2,1.00,0.00,0.50,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,233,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,2,0.00,0.00,0.00,0.00,1.00,0.00,0.00,3,3,1.00,0.00,0.33,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,3,3,0.00,0.00,0.00,0.00,1.00,0.00,0.00,4,4,1.00,0.00,0.25,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## data schema\n",
    "column index\tfeature name\ttype\n",
    "0\tduration\tcontinuous \\n\n",
    "1\tprotocol_type\tdiscrete\n",
    "2\tservice\tdiscrete\n",
    "3\tflag\tdiscrete \n",
    "4\tsrc_bytes\tcontinuous\n",
    "5\tdst_bytes\tcontinuous\n",
    "6\tland\tdiscrete\n",
    "7\twrong_fragment\tcontinuous\n",
    "8\turgent\tcontinuous\n",
    "9\thot\tcontinuous\n",
    "10\tnum_failed_logins\tcontinuous\n",
    "11\tlogged_in\tdiscrete\n",
    "12\tnum_compromised\tcontinuous\n",
    "13\troot_shell\tdiscrete\n",
    "14\tsu_attempted\tdiscrete\n",
    "15\tnum_root\tcontinuous\n",
    "16\tnum_file_creations\tcontinuous\n",
    "17\tnum_shells\tcontinuous\n",
    "18\tnum_access_files\tcontinuous\n",
    "19\tnum_outbound_cmds\tcontinuous\n",
    "20\tis_host_login\tdiscrete\n",
    "21\tis_guest_login\tdiscrete\n",
    "22\tcount\tcontinuous\n",
    "23\tsrv_count\tcontinuous\n",
    "24\tserror_rate\tcontinuous\n",
    "25\tsrv_serror_rate\tcontinuous\n",
    "26\trerror_rate\tcontinuous\n",
    "27\tsrv_rerror_rate\tcontinuous\n",
    "28\tsame_srv_rate\tcontinuous\n",
    "29\tdiff_srv_rate\tcontinuous\n",
    "30\tsrv_diff_host_rate\tcontinuous \n",
    "31\tdst_host_count\tcontinuous\n",
    "32\tdst_host_srv_count\tcontinuous\n",
    "33\tdst_host_same_srv_rate\tcontinuous \n",
    "34\tdst_host_diff_srv_rate\tcontinuous \n",
    "35\tdst_host_same_src_port_rate\tcontinuous \n",
    "36\tdst_host_srv_diff_host_rate\tcontinuous \n",
    "37\tdst_host_serror_rate\tcontinuous \n",
    "38\tdst_host_srv_serror_rate\tcontinuous \n",
    "39\tdst_host_rerror_rate\tcontinuous \n",
    "40\tdst_host_srv_rerror_rate\tcontinuous \n",
    "41\tlabel\tdiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "test_csv_data = test_raw_data.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# get unique value of categorical variables\n",
    "protocols = csv_data.map(lambda x: x[1]).distinct().collect() \n",
    "services = csv_data.map(lambda x: x[2]).distinct().collect()\n",
    "flags = csv_data.map(lambda x: x[3]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '215',\n",
       "  '45076',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.'],\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '162',\n",
       "  '4528',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2',\n",
       "  '2',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1',\n",
       "  '1',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.'],\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '236',\n",
       "  '1228',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '2',\n",
       "  '2',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.50',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.'],\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '233',\n",
       "  '2032',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2',\n",
       "  '2',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '3',\n",
       "  '3',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.33',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.'],\n",
       " ['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '239',\n",
       "  '486',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '3',\n",
       "  '3',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '4',\n",
       "  '4',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.25',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.']]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label point object for train data\n",
    "def create_labeled_point(line_split):# convert string variable to categorical, its value = its index in unique list \n",
    "    # leave_out = [41]\n",
    "    clean_line_split = line_split[0:41]\n",
    "    \n",
    "    # convert protocol to numeric categorical variable\n",
    "    try: \n",
    "        clean_line_split[1] = protocols.index(clean_line_split[1])\n",
    "    except:\n",
    "        clean_line_split[1] = len(protocols)\n",
    "        \n",
    "    # convert service to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[2] = services.index(clean_line_split[2])\n",
    "    except:\n",
    "        clean_line_split[2] = len(services)\n",
    "    \n",
    "    # convert flag to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[3] = flags.index(clean_line_split[3])\n",
    "    except:\n",
    "        clean_line_split[3] = len(flags)\n",
    "    \n",
    "    # convert label to binary label\n",
    "    attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        attack = 0.0\n",
    "    #data=spark.sparkContext.parallelize( np.concatenate((attack, clean_line_split.T), axis=1)) \n",
    "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
    "\n",
    "training_data = csv_data.map(create_labeled_point)\n",
    "test_data = test_csv_data.map(create_labeled_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0,0.0,0.0,0.0,215.0,45076.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,162.0,4528.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,2.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,236.0,1228.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,2.0,2.0,1.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,233.0,2032.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,2.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,3.0,3.0,1.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [0.0,0.0,0.0,0.0,239.0,486.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.0,3.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,4.0,4.0,1.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 286.348 seconds\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "# 3 categorical variable\n",
    "t0 = time()\n",
    "tree_model = DecisionTree.trainClassifier(training_data, numClasses=2, \n",
    "                                          categoricalFeaturesInfo={1: len(protocols), 2: len(services), 3: len(flags)},\n",
    "                                          impurity='gini', maxDepth=4, maxBins=100)\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tree_model.predict(test_data.map(lambda p: p.features))\n",
    "labels_and_preds = test_data.map(lambda p: p.label).zip(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction made in 24.898 seconds. Test accuracy is 0.9159\n"
     ]
    }
   ],
   "source": [
    "# testing accuracy\n",
    "t0 = time()\n",
    "test_accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_data.count())\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 4 with 25 nodes\n",
      "  If (feature 22 <= 33.5)\n",
      "   If (feature 38 <= 0.7949999999999999)\n",
      "    If (feature 36 <= 0.49)\n",
      "     If (feature 34 <= 0.9550000000000001)\n",
      "      Predict: 0.0\n",
      "     Else (feature 34 > 0.9550000000000001)\n",
      "      Predict: 1.0\n",
      "    Else (feature 36 > 0.49)\n",
      "     If (feature 2 in {0.0,5.0,24.0,25.0,14.0,20.0,29.0,1.0,21.0,13.0,2.0,17.0,22.0,27.0,7.0,3.0,11.0,26.0,23.0,8.0,19.0,4.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {0.0,5.0,24.0,25.0,14.0,20.0,29.0,1.0,21.0,13.0,2.0,17.0,22.0,27.0,7.0,3.0,11.0,26.0,23.0,8.0,19.0,4.0})\n",
      "      Predict: 1.0\n",
      "   Else (feature 38 > 0.7949999999999999)\n",
      "    If (feature 3 in {0.0,1.0,6.0,2.0,8.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 3 not in {0.0,1.0,6.0,2.0,8.0})\n",
      "     If (feature 36 <= 0.44)\n",
      "      Predict: 1.0\n",
      "     Else (feature 36 > 0.44)\n",
      "      Predict: 1.0\n",
      "  Else (feature 22 > 33.5)\n",
      "   If (feature 5 <= 2.0)\n",
      "    If (feature 2 in {11.0,66.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 2 not in {11.0,66.0})\n",
      "     If (feature 11 <= 0.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 11 > 0.5)\n",
      "      Predict: 0.0\n",
      "   Else (feature 5 > 2.0)\n",
      "    If (feature 29 <= 0.08499999999999999)\n",
      "     If (feature 2 in {0.0,5.0,10.0,1.0,2.0,22.0,8.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 2 not in {0.0,5.0,10.0,1.0,2.0,22.0,8.0})\n",
      "      Predict: 1.0\n",
      "    Else (feature 29 > 0.08499999999999999)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned classification tree model:\")\n",
    "print(tree_model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service 0 is http\n",
      "Service 52 is pop_2\n"
     ]
    }
   ],
   "source": [
    "print(\"Service 0 is {}\".format(services[0]))\n",
    "print(\"Service 52 is {}\".format(services[52]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree_model.save(sc, \"./myDecisionTreeClassificationModel\")\n",
    "#sameModel = DecisionTreeModel.load(sc, \"./myDecisionTreeClassificationModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference \n",
    "# https://github.com/sanxofon/json_html_viewer/blob/master/json2js/json2jsondata.py\n",
    "# https://github.com/tristaneljed/Decision-Tree-Visualization-Spark\n",
    "\n",
    "# Parser\n",
    "def parse(lines):\n",
    "    block = []\n",
    "    while lines:\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if lines[0].startswith('If'):\n",
    "            bl = ' '.join(lines.pop(0).split()[1:]).replace('(', '').replace(')', '')\n",
    "            block.append({'name':bl, 'children':parse(lines)})\n",
    "            if lines[0].startswith('Else'):\n",
    "                be = ' '.join(lines.pop(0).split()[1:]).replace('(', '').replace(')', '')\n",
    "                block.append({'name':be, 'children':parse(lines)})\n",
    "        elif not lines[0].startswith(('If','Else')):\n",
    "            block2 = lines.pop(0)\n",
    "            block.append({'name':block2})\n",
    "        else:\n",
    "            break\n",
    "    return block\n",
    "\n",
    "# Convert Tree to JSON and save to structure.js file\n",
    "def tree_json(tree):\n",
    "    data = []\n",
    "    for line in tree.splitlines() : \n",
    "        if line.strip():\n",
    "            line = line.strip()\n",
    "            data.append(line)\n",
    "        else : break\n",
    "        if not line : break\n",
    "    res = []\n",
    "    res.append({'name':'Root', 'children':parse(data[1:])}) # the recursive function parse each line of the result \n",
    "    #file = 'structure =' + str(res[0])\n",
    "    with open('./structure.json', 'w') as outfile: # write tree as json structure to file\n",
    "        json.dump(res[0], outfile)\n",
    "    with open('./structure.json', 'r') as outfile: # open json file get data\n",
    "        data=outfile.read()\n",
    "    with open(\"./structure.js\",\"w\") as outfile: # rewrite as js file for visualization\n",
    "        outfile.write(\"structure = '\"+data +\"';\")\n",
    "\n",
    "    print ('Conversion Success !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Success !\n"
     ]
    }
   ],
   "source": [
    "## Create strucure.js file with the full-tree result\n",
    "tree_json(tree_model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Minimal Model Using the Three Main Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labelpoint object for minimal tree with 6 features\n",
    "\n",
    "def create_labeled_point_minimal(line_split):\n",
    "    # leave_out = [41]\n",
    "    # only use feature [2,3,5,22,36,38]\n",
    "    clean_line_split = line_split[2:4] + line_split[5:6] + line_split[22:23]+ line_split[36:37]+ line_split[38:39]\n",
    "    \n",
    "    # convert flag to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[0] = services.index(clean_line_split[0])\n",
    "    except:\n",
    "        clean_line_split[0] = len(services)\n",
    "        # convert flag to numeric categorical variable\n",
    "    try:\n",
    "        clean_line_split[1] = flags.index(clean_line_split[1])\n",
    "    except:\n",
    "        clean_line_split[1] = len(flags)\n",
    "    \n",
    "    # convert label to binary label\n",
    "    attack = 1.0\n",
    "    if line_split[41]=='normal.':\n",
    "        attack = 0.0\n",
    "        \n",
    "    return LabeledPoint(attack, array([float(x) for x in clean_line_split]))\n",
    "\n",
    "training_data_minimal = csv_data.map(create_labeled_point_minimal)\n",
    "test_data_minimal = test_csv_data.map(create_labeled_point_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_minimal.take(5)\n",
    "len(services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained in 188.266 seconds\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "t0 = time()\n",
    "tree_model_minimal = DecisionTree.trainClassifier(\n",
    "    training_data_minimal, numClasses=2, \n",
    "    categoricalFeaturesInfo={0: len(services),1: len(flags)},\n",
    "    impurity='gini', maxDepth=3, maxBins=70)\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Classifier trained in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction made in 15.666 seconds. Test accuracy is 0.9184\n"
     ]
    }
   ],
   "source": [
    "predictions_minimal = tree_model_minimal.predict(test_data_minimal.map(lambda p: p.features))\n",
    "labels_and_preds_minimal = test_data_minimal.map(lambda p: p.label).zip(predictions_minimal)\n",
    "\n",
    "### Check the testing accuracy\n",
    "t0 = time()\n",
    "test_accuracy = labels_and_preds_minimal.filter(lambda x: x[0] == x[1]).count() / float(test_data_minimal.count())\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Prediction made in {} seconds. Test accuracy is {}\".format(round(tt,3), round(test_accuracy,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification minimal tree model:\n",
      "DecisionTreeModel classifier of depth 3 with 15 nodes\n",
      "  If (feature 3 <= 33.5)\n",
      "   If (feature 5 <= 0.7949999999999999)\n",
      "    If (feature 4 <= 0.49)\n",
      "     Predict: 0.0\n",
      "    Else (feature 4 > 0.49)\n",
      "     Predict: 1.0\n",
      "   Else (feature 5 > 0.7949999999999999)\n",
      "    If (feature 1 in {0.0,1.0,6.0,2.0,8.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 1 not in {0.0,1.0,6.0,2.0,8.0})\n",
      "     Predict: 1.0\n",
      "  Else (feature 3 > 33.5)\n",
      "   If (feature 2 <= 2.0)\n",
      "    If (feature 0 in {11.0,66.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 not in {11.0,66.0})\n",
      "     Predict: 1.0\n",
      "   Else (feature 2 > 2.0)\n",
      "    If (feature 0 in {0.0,2.0,8.0,10.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 not in {0.0,2.0,8.0,10.0})\n",
      "     Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned classification minimal tree model:\")\n",
    "print(tree_model_minimal.toDebugString())\n",
    "# feature 0: Services, 1: flag, 2: dst_bytes, 3: count, 4: dst_host_srv_diff_host_rate, 5: dst_host_srv_serror_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the minimal tree result to \"structure.js\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion Success !\n"
     ]
    }
   ],
   "source": [
    "# create structure.js file with the result from minimal tree\n",
    "tree_json(tree_model_minimal.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service 11 is urp_i\n",
      "Service 66 is tftp_u\n",
      "Flags 0:2 is ['SF', 'S2', 'S1']\n",
      "Flags 6 is RSTO\n",
      "Flags 8 is RSTR\n",
      "Flags 10 is SH\n"
     ]
    }
   ],
   "source": [
    "# Get label of freature to interpret the tree\n",
    "print(\"Service 11 is {}\".format(services[11]))\n",
    "print(\"Service 66 is {}\".format(services[66]))\n",
    "print(\"Flags 0:2 is {}\".format(flags[0:3]))\n",
    "print(\"Flags 6 is {}\".format(flags[6]))\n",
    "print(\"Flags 8 is {}\".format(flags[8]))\n",
    "print(\"Flags 10 is {}\".format(flags[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
